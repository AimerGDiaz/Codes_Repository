---
title: "Scripts and Oneliners explained"
author: "Aimer G. Diaz"
output: github_document
---
<!---
Github extras 

https://github.com/gayanvoice/github-profile-views-counter

Sintaxis propia de github markdown https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet

Sintaxis for all the R markdowns in general https://bookdown.org/yihui/rmarkdown-cookbook/raw-latex.html 
https://bookdown.org/yihui/rmarkdown/language-engines.html

El tema de las licencias https://gist.github.com/lukas-h/2a5d00690736b4c3a7ba

Cuando lanze los pauetes tanto de deteccion de fragmentos como el script de reduccion de librerias https://docs.github.com/en/enterprise-server@2.22/packages/quickstart  
https://github.com/LorenzoTa/step-by-step-tutorial-on-perl-module-creation-with-tests-and-git/blob/master/tutorial/tutorial-english.md
https://fpm.readthedocs.io/en/v1.13.1/intro.html
https://github.com/jordansissel/fpm/pull/876

git hub pulling pushing and difference https://github.blog/2011-10-21-github-secrets/ 

SQL too https://www.red-gate.com/hub/product-learning/sql-source-control/github-and-sql-source-control 

Wikis en github https://guides.github.com/features/wikis/

--->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#devtools::install_github("hadley/emo")

```
Welcome to my code repository, most or all of it I've learned in a very inclusive sense of the word (including copy from internet forums), following this beautiful logic of public domain I've made these notes available, the idea it's to have a server-independent repository, a repository on the cloud, or should I say, on the bottom of the ocean with several copies in several sites of the world, for today and maybe, just maybe, even available on a post-global ecological disaster era, available physically but not online. 

I will try to indicate here the structure of all the folders of this repository, each folder anyway will have more information with its own R markdown. The idea of this out of site file is to indicate meta-folders features, like links for indicate where the same code is used in the exactly same way, or when I made a change for any reason for a specific project (all of them would be on bioinformatics next-generation data analysis, my main expertise). 

As an example a very basic but useful code I wrote in bash syntax and I use it for every project, it's a code I call `sra_download.sh`, which take a given list (or even better with the SRA Run selector table  from Sequence Read Archive (SRA)) and download differentiating, if the input table indicate, if the fastq file is single or paired end. Let's explore the latest data set I was working when I wrote this document, from the [SRA site](https://www.ncbi.nlm.nih.gov/Traces/study/?acc=SRP190362&o=acc_s%3Aa): 

SRA_Code | Sequence_Tech | Cell_location_stage | Strain | Total_Sequences | Length | Source | ServerName
--- | --- | --- | --- | --- | --- | --- | ---
SRR8859642 | PAIR END | Procyclic | EATRO 1125 | 85,827,714 | 125/125 | Cooper2019_gRNA | Cooper2019_gRNA_E1125PC
SRR8859640 | PAIR END | Bloodstream | EATRO 1125 | 79,132,567 | 125/125 | Cooper2019_gRNA | Cooper2019_gRNA_E1125BS
SRR8859641 | PAIR END | Bloodstream + Procyclic MIX | EATRO 1125 | 148935513 | 125/125 | Cooper2019_gRNA | Cooper2019_gRNA_E1125Mix

The first version of the file is [on the master thesis codes folder](Total_processing/Download_script.sh), however this version did not has the option to download the SRA considering if it's single or paired end. The latest version of it does and it's on the [Trypanosoma repository](https://github.com/AimerGDiaz/Trypanosomes_RNA_editing).

***

# Codes repository

In order to start, I will comment here sections of the codes and the way I apply them to solve specific problems, in other terms, here what you will find is a selection of commented and finished codes in bash, awk, perl, python, R and Latex languages, plus R markdowns files explaining these codes using small toy data sets. Finally in this document you can also find useful one liners commented, also with toy examples. 

Let's start with the one-liners, it's true they are not the best solution, however they are fast, they avoid us to write or re-adapt whole blocks of code to particular solutions, they are modular and somehow, they are like culture, beyond if it's recommended or not to use them, some people love them and I am one of those. I will introduce them first, because I always started with them to explore the original data, already made analysis, new formats, supplementary material, etc. 

## Awk 

Awk codes are my favorite, the one liners are extreamlly useful, fast, weird but incredible, but they can be adapted as bash codes and even using in parallel computing by the amazing GNU parallel software with minor modifications as I will show here. But let's start, the selected awk codes and toy data samples will be [here](AWK/README.md). 

### Generalities of my awk codes 

All the languague I've mentioned before will have the same structure for this section: main applications of the main control structures (<for>, <while> and <if>),  combinations of control structures and most employed codes so far. Before enter to control structres, there are awk specific features important to highlight: 

* Non line per line analysis: Awk can work not only line per line, it's able to integrate as a unit of analysis files which format might have a specific set of lines per element, for instance 2 lines describe a single sequence in a fasta format or 4 lines a fastq file, how to integrate those lines to a single unit of analysis?. The `getline` awk function it's a faster way to make it, meaning very simple and useful one-liners of awk might be used for files like this. 

First example a Fastq to Fasta file converter using the next one-liner: 

```{bash eval=FALSE}
awk ' BEGIN {OFS = "\n"} {header = $0 ; getline seq ; getline qheader ; getline qseq ; print ">"header,seq}' $1 > $name.fa
```
A very similar task can be achieved with a derivative code, like change the headers of multi-fasta file for a shorter version of it or simply to add the sequence length for blast output filters, which can be achieved using: 

```{bash eval=FALSE}
awk  ' BEGIN {OFS = "\n"} {header = $0 ; getline seq  ; gsub(/;Ant.*/,"",header); header=header";length="length(seq);  print header,seq}' $1 > $name.fa
```

This code is integrated to a function of change suffix on the code called [fq2fa.sh](AWK/fq2fa.sh). The function getline must be call as many line we want to integrated in a single unity of analysis. 

*** 
* Change delimiters, awk regrex 101 : a related issue to the previous one it's the case when you want to change the field delimiter of certain lines, but not all, a typical example in bioinformatics of this task it's transform a multi-fasta file with multiple jump lines into a single line per sequence, graphically: 

From 

`>1
ACCCAGAGAGTGAG
ACCAACACACAGTT`

To 

`>1
ACCCAGAGAGTGAGACCAACACACAGTT`

To make it we can use the regrex expression integrated with the simplified if control loop of awk: 
```{bash eval=FALSE}
awk '/^>/ {printf("\n%s\n",$0);next; } { printf("%s",$0);}  END {printf("\n");}' $1 > $1.ol.fa 
```
On this awk-oneliner, we first select those lines who do start with > `/^>/` anything in between would be separated only by one jump of line per each `^>` line found. This code it's called [oneliner.awk](AWK/oneliner.awk)

***
#### If loops

* IF in arrays as a classificator: One of my favorite codes made a classificatory tasks extremely fast and with just a handful set of commands, sadly the economy of the codes makes it a little dark, especially for AWK beginners.  

<details> 
<summary>Let's see a toy example:</summary>

```{bash, include=FALSE}
rm -f AWK/td_Gene_duplication_per.txt 
```
```{bash}
#We can create a toy data (td) set  with a chromosome location of a given gene

#Chromosome  #Duplicated Gene
#chr1       geneA
#chr2       geneB
#chr3       geneA

echo -ne "Chromosome\tDuplicated Gene\nchr1\tgeneA\nchr2\tgeneB\nchr3\tgeneA\n" > AWK/td_Gene_duplication_per.txt 

```
</details>

Sadly, although it seems awk engine is implemented for R markdown, it requires an additional effort to make it work, here the code of how to run awk using [knitr](https://github.com/yihui/knitr-examples/blob/master/024-engine-awk.Rmd), and here the [R markdown output](https://github.com/yihui/knitr-examples/blob/master/024-engine-awk.md) of that code, however after several attempts I could not make awk work here, anyway, awk is integrated as a command on bash, then we can write the comand of awk as a awk script and executing with bash. 
<details> 
<summary>The code is</summary>
```{bash}
head -n 25 AWK/classificator.awk 
``` 
</details> 

<details> 
<summary>The output of this script</summary>

```{bash}
bash AWK/classificator.awk AWK/td_Gene_duplication_per.txt
```
</details> 

<details> 
<summary> What happen if we include more genes ? </summary>

```{bash}
echo -ne "chr4\tgeneA" >> AWK/td_Gene_duplication_per.txt 

bash AWK/classificator.awk AWK/td_Gene_duplication_per.txt

```
```{bash, include=FALSE}
rm -f AWK/td_Gene_duplication_per.txt 
```
</details> 

Now let's see this code applied to real world problems, at least how I used, one example is here 
PRINT here how it work to make quick table (Mirnomics project) or for multi-fasta file duplication cleaning (gRNAs_total.fa)

DEVELOP HERE 

***
* IF and arrays as de-duplicated control structures: 

DEVELOP HERE 

#### For and While 
***
* While, beigin honest I have never used while loop on awks. But I do use while structure on bash followed by an awk code, which is, awk as line per line variable mining tool, which employ the next code structure: 

```{bash}
echo  "This a toy example; with a semicolon separator structure; and I want to show you; hidden variable X 
that using bash while loop; and awk -F separator command; I can extract pf each line the; hidden variable Y" > toy_structure.txt

head toy_structure.txt
```
```{bash}
counter=1   ### Bash counter - reference here for access from other sites
while read line 
do 
variable=`echo $line | awk -F'; hidden variable' '{print $2}'`
echo this is the value of the hidden variable: "$variable" on the line: "$counter" 
let counter=counter+1 
done < toy_structure.txt 

rm -f toy_structure.txt
```
A real world problem where I use awk inside a bash while loop structure is in a fastqc quality information miner script, extracted from [NGS_statistics.sh script](Total_processing/NGS_statistics.sh):
<details> 
<summary> Non excecutable script </summary> 
```
 while read library
 do
 FILE="$1"/Fastq/Quality/$library/fastqc_data.txt
 
 type_file=`echo $library | egrep -o  "_fastqc|_reduced_cor_fastqc|.reduced_fastqc|_trim_fastqc"`
 
 name=`echo $library | awk -v sus="$type_file"  '{gsub(sus,"",$0);print $0}'`

 if [ "$type_file" == "_fastqc" ]
 then
 lib_type="raw"
 elif [ "$type_file" == "_reduced_cor_fastqc" ]
 then
 lib_type="ms_reduced"
 elif [ "$type_file" == ".reduced_fastqc" ]
 then
 lib_type="reduced"
 elif [ "$type_file" == "_trim_fastqc" ]
 then
 lib_type="tmm_reduced"
 fi

 awk '/^Total Sequences/{ts=$3}  /^Sequence length/{print "'$name'" "\t" "'$lib_type'" "\t" $3 "\t" ts}' $FILE   >> $1"Results/Statistics/Total_size.txt"

 done < dir_list.txt
```
</details> 
***
* For loops on awk are mainly array handlers: 

DEVELOP HERE 

***

## Bash  

Here as well as with awk there are many commands that using cleverly you can get results with a single one-liner, or post process, clean, adjust ins/outs quickly or to change formats to give to more complicated programs. Bash it's for me, as you saw previously, the main executor, for other people it could be shell, or even anything without command line, by running all in environments like this, but with VIM + Bash + awk + perl virtually you could do everything on structured programming. But before to get into the details of how I used recurrent blocks of code, I will introduce a unique aspect of bash: 


### Generalities of my bash codes 

Bash as well as AWK is quite flexible, however most of the time I do use the three control structures `for`, `while` and `if, then, else` for data organization, classification or text processing and quality control. But before to enter to each control structure let's talk some other features of Bash beyond the control statements itself. 

* Piping on bash: beyond "|"

Bash is awesome specially when we talk about pipes, input and output immediately re-direction. However for beginners the main command for piping is usually "|", which limits piping to comands who tolerate input redirection in such way, however there are software which cannot be piped with "|" but it might accept "-" for input file declaration, specially those who requires a -i parameter to indicate the input file, then the piping way for such programs is " | software -i - " . 

DEVELOP HERE - example of "-" for piping 

A less usual piping commands is the combined operator "<( code here )", which could be interpreted as "take the output of the command between parenthesis as an input". This structure could be used to expand the capabilities to of grep command but in the option of grepping a list of terms (grep -f). Using the -f parameter on a grep search we cannot add the single pattern options available also as parameter, for instance, searching the list of pattern at the beginning of each line: `grep -f "^" `, however using `<()` this task it's possible. Let's explore using a real world problem. 

In a file of small RNAs who target genes for mRNA edition process (U deletion or insertion), each line represent a guide RNA, whoever some of them has complex gene target assignation, meaning for instance like: 

"Unknown_RPS12"

Where the first category means the guideRNA have gotten a gene target after a secondary process, but a gene without a gene target even after applying the secondary process, could still lack of a gene assignation. Additional  patterns like RPS12_Unknown_RPS12, make even harder the use of auxiliary awk syntax. Therefore, to quantify the exact amount of gRNAs with pure Unknown gene targets we can use the `<()` syntax as follow:  


First generate the pattern to search using the single line grep regular expressions as "^" or "$" or others, for instance starting with a file like this: 
```{bash}
head -n 1 BASH/grep_lists_example.txt
```               
Where each line represent a single ID, now as this pattern in the file we want to search it might be place on more than one column, a exact grep (-w) search would not be enough, the file to search looks like:
```{bash}
head -n 1 BASH/file2search.txt
```
The pattern we are interested is the first field of this comma separated file, as it's numerical it might happen in many places, then what we need to search is a list of ids who start exactly with that number and ends with a comma, we can create a list of IDs with these features: 
```{bash}
bash AWK/printBeginningEnd.awk BASH/grep_lists_example.txt  > BASH/tempids.sh

head -n 1 BASH/tempids.sh
```
With awk basically we have created a script who writes another script, which tells to bash how to print the code, if we excecute this code it will look as follow:
```{bash}
bash BASH/tempids.sh | head -n 2
```
Now with this script we can run the line `bash tempids.sh` inside `<()` command as an input for the command `grep -f`: 
```{bash}
time (bash BASH/piping.sh BASH/file2search.txt)
head BASH/piping.sh
```
In such way this code is equivalent to run grep in a for loop as: 

```{bash}
bash BASH/piping_alternative.sh BASH/file2search.txt
head BASH/piping_alternative.sh
```
However the first code is much faster as time command show us, the reason of this is because the first is in reality a single grep search without control structures. 

The second code has the bash array structure and the way it can be feed it, also as an alternative to avoid temporary files. Also includes how we can access or called in a bash for loop, [more of bash arrays  here](https://opensource.com/article/18/5/you-dont-know-bash-intro-bash-arrays). 

### Creating comands on Bash 

* Save routinary functions and alias as bash commands with the label you want to use it for them on the terminal. To start, what we need it's edit the file `~/.bashrc` with your favorite text editor, in my case Vim ([(guide for vim commands)](VIM/README.md)). The difference between alias and the functions is mainly that the functions read arguments, while alias not. My favorite list of alias and functions on .bashrc file

<details>
<summary> * Alias </summary>

```{bash}
# List the files in human readble format withut writing -lh and
# organize them by size, from bigger to smaller
alias lso='ls -lh  --sort=size'

# List the files in human readble format withut writing -lh and 
# organize them by time of modification, from sooner to later 
alias lst='ls -lh  --sort=time'

# Delete the files but asking first if that's what you really want 
alias rm='rm -i'
# anyway rm -f would delete without asking, but it requires from you an extra
# time thinking 

# I usually work on servers, I do not like to write ssh hostname@server thousands of time
alias msuhpc=' ssh -X USER@SERVER'

# First time on the matrix? sometimes is hard to know if you are working on a screen session 

alias iscreen=' echo $TERM ' 

# If you are on a server always work on screens, if you lose connection, the screens nope, 
# but outside the matrix, the world is harder. 

# Again an useful alias when you do not want to be kick out from the server, but you are not
# mentally there 

alias waste_time='for f in {1..200}; do echo ------ wasting time minutes $f;  sleep 60; done'
```
</details>

<details>
<summary> * Functions </summary>

Alias are boring, but save time. Functions are quite interesting, they are basically mini software and bash allow you to create commnads as variated as a blast of a sequence you want to test against blast databases. 

```{bash}
# LiSt the Absolut path Directory of a file 
lsad() {
find $PWD -iname $1
}

# List the size of a folder
lsf(){
du -ch --max-depth=0  $1
}

# Search on history a past command 
hisgrep() {
history | grep $1
}

# What if you use a file many times and the previous command shows many boring history
# You can also check only when the file you are interested in was generated 
hisgrep_gen() {
hisgrep "-P \>\\s+$1"
}

# That's all for today darling screen 
delete_screen() {
screen -X -S $1 quit
}


# Adjust the title of a paper to my format of saving it 
papers() {
        final=`echo "$1" | tr '\n' '_' | tr -d 'â€“' |tr ' ' '_' | tr -d ',' |tr -d ':' | tr -d '(' | tr -d ')' | tr '/' '-' | awk -F '_$' '{gsub("__","_",$1);print $1}' | awk -F '_$' '{print $1}' `
echo $2.$final
}

# How many nucleotides do a sequence has
count_nt() {
seq=`echo -ne $1 | wc -c`
echo $seq nucleotides
}

# Get the reverse complementary sequence of a 
revcom() {
 if [ "$1" == "DNA" ]
 then
 echo $2 | rev | tr 'ACGTacgt' 'TGCAtgca'
 elif [ "$1" == "RNA" ]
 then
 echo $2 | rev | tr 'ACGUacgu' 'UGCAugca'
 fi
 }

# Check how large is the load of a server
server_status() {
ssh server free -h
ssh server  ps aux --sort=-pcpu | egrep -v "root" | head # add any annoying software always coming up from ps 
ssh server  ps aux --sort=-pcpu   | awk 'NR>1{sum+=$3;sum2+=$4}END{print "Cores used "sum "% and RAM used "sum2" %"}'
}

# Search in a given directory a file by its pattern
reference(){
pattern=`echo $1`
find $2 -iname "*$pattern*"  -print 2>/dev/null 
}

# Search a word in a pdf o thousands of pdfs on a common directory or a single file
# Of course it does requires the installation of pdfgrep
intext(){
find "$1" -name '*pdf' -exec pdfgrep -i "$2" /dev/null {} \; 2>/dev/null
}
```
</details>

These newly defined codes can be used even into common scripts, but first it requires installation: 

```{bash}
source ~/.bashrc
```
To running here, I need to use these commands into scripts, and then you must use the `-i` parameter, which means: 

`-i        If the -i option is present, the shell is interactive.`

Let's see an example, tell me computer how looks the reverse complementary sequence of ACCCCGAGACTAGGTAGAGACA and how many nucleotides are there?: 

This is how looks the script for this: 

```{bash}
head BASH/running_bashrc.sh
```
... And here the output: 
```{bash}
bash -i  BASH/running_bashrc.sh
```
*** 
## Perl

Perl ate awk once in iths human evolutioanry history, 
<!-- Usar perl en R markdown, se puede https://stackoverflow.com/questions/45857934/executing-perl-6-code-in-rmarkdown 
--->

## License 

My codes are licensed under a [Creative Commons Attribution-ShareAlike 4.0 International License](https://creativecommons.org/licenses/by-nc/4.0/).

[![License: CC BY-NC 4.0](https://img.shields.io/badge/License-CC%20BY--NC%204.0-lightgrey.svg)](https://creativecommons.org/licenses/by-nc/4.0/)

## Viewers 

[![Image of Viewers](https://github.com/AimerGDiaz/Viewers/blob/master/svg/409164432/badge.svg)](https://github.com/AimerGDiaz/Viewers/blob/master/readme/409164432/week.md)

